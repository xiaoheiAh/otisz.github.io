<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Redis HA - 哨兵模式 - xiaoheiAh's blog</title><link rel=icon type=image/png href=https://blog.xiaohei.im/favicon.ico><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content="Redis HA - 哨兵模式"><meta property="og:description" content="「Redis 学习笔记」| HA | 高可用 | 哨兵模式 | sentinel"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.xiaohei.im/posts/redis/sentinel/"><meta property="article:published_time" content="2019-11-23T17:56:15+08:00"><meta property="article:modified_time" content="2019-11-23T17:56:15+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Redis HA - 哨兵模式"><meta name=twitter:description content="「Redis 学习笔记」| HA | 高可用 | 哨兵模式 | sentinel"><link rel=stylesheet type=text/css media=screen href=https://blog.xiaohei.im/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://blog.xiaohei.im/css/main.css><link rel=stylesheet href="https://fonts.loli.net/css2?display=swap&family=Bitter&family=Noto+Sans+SC"><link rel=stylesheet type=text/css href=https://blog.xiaohei.im/css/dark.css media="(prefers-color-scheme: dark)"><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script><script src=https://blog.xiaohei.im/js/main.js></script></head><body><div class="container wrapper post"><div class=header><h1 class=site-title><a href=https://blog.xiaohei.im/>xiaoheiAh's blog</a></h1><div class=site-description><h2>Java Developer | 关注后端</h2><nav class="nav social"><ul class=flat><a href=https://github.com/xiaoheiAh title=xiaoheiAh><i data-feather=github></i></a></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/awesome>Awesome</a></li><li><a href=/cheatsheet>CheatSheet</a></li><li><a href=/about>About</a></li></ul></nav></div><div class=post-header><h1 class=title>Redis HA - 哨兵模式</h1><div class=meta style=display:inline>Posted at &mdash; Nov 23, 2019</div><div class=meta style=display:inline><a>Page Views: 2</a></div></div><div class=markdown><p>Redis 官方高可用(HA)方案之一: <strong>哨兵模式</strong></p><p>这篇文章已经介绍的很全面了:https://juejin.im/post/5b7d226a6fb9a01a1e01ff64 自己就总结一些问题:</p><h2 id=sentinel-如何保证集群高可用>sentinel 如何保证集群高可用?</h2><ol><li>时刻与监控的节点保持心跳(PING),订阅 <code>__sentinel__:hello</code> 频道实时更新配置并持久化到磁盘</li><li>自动发现监听节点的其他 <code>sentinel</code> 保持通信</li><li>节点不可达时询问其他节点确认是否不可达,是否需要执行故障转移(半数投票)</li><li>故障转移后广播配置,帮助其他从节点切换到新的主节点,以 <code>epoch</code> 最大的配置为准</li></ol><h2 id=sentinel-如何判定节点下线>sentinel 如何判定节点下线?</h2><p><a href=#%E4%B8%BB%E8%A7%82%E4%B8%8B%E7%BA%BF/%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF>主观下线/客观下线</a></p><h2 id=sentinel-的局限性>sentinel 的局限性?</h2><p><code>Redis Sentinel</code> 仅仅解决了 <strong>高可用</strong> 的问题，对于 <strong>主节点</strong> 单点写入和单节点无法扩容等问题，还需要引入 <code>Redis Cluster</code> <strong>集群模式</strong> 予以解决。</p><h2 id=官方文档介绍>官方文档介绍</h2><blockquote><p><a href=https://redis.io/topics/sentinel>https://redis.io/topics/sentinel</a></p><p>使用 <code>sentinel</code> 的原因: 做到无人工介入的自动容错 <code>redis</code> 集群.</p></blockquote><p><code>sentinel</code> 宏观概览:</p><ul><li><strong>监控 Monitoring:</strong> 持续监控主从节点的运行状态</li><li><strong>通知 Notification:</strong> 节点异常时,通过暴露的 <code>API</code> 可以及时报警</li><li><strong>自动故障转移 Automatic Failover:</strong> 主节点宕机后,可以自动晋升从节点为新主节点,其他节点会重新连接到新主节点,应用也会被通知相应的节点变化</li><li><strong>提供配置 Configuration Provider:</strong> <code>sentinel</code> 维护着主从的节点信息,客户端会连接<code>sentinel</code> 获取主节点信息.</li></ul><p><code>sentinel</code> **天生分布式,**多节点协同的好处在于:</p><ul><li>多数节点都同意主节点不可用时才执行故障检测.有效避免错判.</li><li>多节点可以提升系统鲁棒性(<code>system robust</code>),避免单点故障</li></ul><h3 id=使用须知>使用须知</h3><ol><li>至少 3 个 <code>sentinel</code> 保证系统鲁棒性</li><li>节点最好放在不同的主机或虚拟机,降低级联故障(一下全GG)</li><li>由于 <code>redis</code> 采用的是异步复制,<code>sentinel</code> + <code>redis</code> 不能保证故障期间确认的写入(主从可能无法通信,确认复制进度).<code>sentinel</code> 可以在发布时控制一定时间内数据不丢失,但也不是万全之策.</li><li>客户端需要支持 <code>sentinel</code> (常用 <code>Java</code> 客户端基本都支持)</li><li>高可用并不是百分之百有效,即时你时时刻刻都在测试,产线环境也在跑,保不准凌晨就 GG,也没办法不是.</li><li><code>Sentinel</code>,<code>Docker</code>，或者其他形式的网络地址交换或端口映射需要加倍小心：Docker执行端口重新映射，破坏<code>Sentinel</code>自动发现其他的哨兵进程和主节点的 <code>replicas</code> 列表。</li></ol><h3 id=sentinel-设置>Sentinel 设置</h3><p><code>redis</code> 安装目录下有一个 <code>sentinel.conf</code> 模板配置可以参考.最小化配置如下:</p><div class=highlight><pre style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain>sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 60000
sentinel failover-timeout mymaster 180000
sentinel parallel-syncs mymaster 1

sentinel monitor resque 192.168.1.3 6380 4
sentinel down-after-milliseconds resque 10000
sentinel failover-timeout resque 180000
sentinel parallel-syncs resque 5
</code></pre></div><p>不需要配置 <code>replicas</code> , <code>sentinel</code> 可以自动从主节点中获取 <code>INFO</code> 信息.同时该配置也会实时重写的: 新的 <code>sentinel</code> 节点加入或者故障转移 <code>replica</code> 晋升时.</p><h4 id=sentinel-monitor-master-group-name-ip-port-quorum><code>sentinel monitor &lt;master-group-name> &lt;ip> &lt;port> &lt;quorum></code></h4><p>从命令就可以看出来一些名堂: 监控地址为 <code>ip:port,name</code> 为 <code>master-group-name</code> 的主节点.</p><p><strong>quorum:</strong> 判断节点确实已经下线的支持票数(由 <code>Sentinel</code> 节点进行投票),票数超过一定范围后就可以让节点下线并作故障转移.但 <code>quorum</code> 只是针对于下线判断,执行故障转移需要在 <code>sentinel</code> 集群选举(投票)出一个 <code>leader</code> 来执行故障转移.</p><p>e.g. <code>quorum</code> = 2, <code>sentinel</code> 节点数 = 5</p><ul><li>如果有两个 <code>sentinel</code> 节点认为主节点下线了,那么这两个节点中的一个会尝试开始执行故障转移.</li><li>如果有超过半数 <code>sentinel</code> 节点存在(当前情况下即活着 3 个 <code>sentinel</code> 节点),故障转移就会被授权真正开始执行.</li></ul><p><strong>核心概念:</strong> <code>sentinel</code> 节点半数不可达就不允许执行 <strong>故障转移</strong>.</p><h4 id=sentinel-option_name-master_name-option_value><code>sentinel &lt;option_name> &lt;master_name> &lt;option_value></code></h4><p><code>sentinel</code> 其它的配置基本都是这个格式.</p><ul><li><code>down-after-milliseconds</code> 节点宕机超过该毫秒时间后 <code>sentinel</code> 节点才能认为其不可达.</li><li><code>parallel-syncs</code> 在发生failover主从切换时，这个选项指定了最多可以有多少个 <code>replica</code> 同时对新的<code>master</code> 进行同步，这个数字越小，完成主从故障转移所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为主从同步而不可用。可以通过将这个值设为1来保证每次只有一个 <code>replica</code> 处于不能处理命令请求的状态。</li></ul><p><strong>所有配置都可以通过 <code>SENTINEL SET</code> 热更新.</strong></p><h4 id=添加删除-sentinel-节点>添加/删除 sentinel 节点</h4><p><strong>添加</strong>: 启动一个新的 <code>sentinel</code> 即可.10s就可以获得其他 <code>sentinel</code> 节点以及主节点的 <code>replicas</code> 信息了.</p><p><strong>多节点添加</strong>:建议 <code>one by one</code>,等到当前节点添加进集群后,再添加下一个.添加节点过程中可能会出故障.</p><p><strong>删除节点:</strong> <code>sentinel</code> 节点不会丢失见过 <code>sentinel</code> 节点信息,即使这些节点已经挂了.所以需要在没有网络分区的情况下做以下几步:</p><ol><li>终止你想要关掉的 <code>sentinel</code> 节点进程</li><li>发送一条命令 <code>SENTINEL RESET *</code> 给所有 <code>sentinel</code> 节点.如果只想对单一 <code>master</code> 处理,把 <code>*</code> 换成主节点名称.等一会儿~</li><li>通过 <code>SENTINEL MASTER</code> 命令查看节点是否已删除</li></ol><h4 id=主观下线客观下线>主观下线/客观下线</h4><p><code>sentinel</code> 中有两种下线状态.</p><ul><li><p><strong>主观下线(Subjectively Down)</strong> aka. SDOWN</p><p>当前 <code>sentinel</code> 认为自己监控的节点下线了,即主观下线.<code>SDOWN</code> 判定的条件为: <code>sentinel</code> 节点向监控节点发送 <code>PING</code> 命令在设置的 <code>is-master-down-after-milliseconds</code> 毫秒后没有收到有效回复则判定为 <code>SDOWN</code></p></li><li><p><strong>客观下线(Objectively Down)</strong> aka. ODOWN</p><p>有 <code>quorum</code> 数量的 <code>sentinel</code> 节点认为监控的节点 <code>SDOWN</code>.当一个 <code>sentinel</code> 节点认为监控的节点 <code>SDOWN</code> 后,会向其它节点发送 <code>SENTINEL is-master-down-by-addr</code> 命令来判断其它节点对该节点的监控状态.如果回执为 <strong>已下线</strong> 的节点数+自身大于 <code>quorum</code> 数量,则判定为 <strong>客观下线</strong></p></li></ul><p><code>PING</code> 命令的有效回复有什么?</p><ul><li>+PONG</li><li>-LOADING error</li><li>-MASTERDOWN error</li></ul><p>其它回复都是无效的.需要注意的是: 只要收到有效回复就不会认为其 <code>SDOWN</code> 了.</p><p><code>SDOWN</code> 并不能触发故障转移,只能判定节点不可用.要触发故障转移,<strong>必须</strong>达到 <code>ODOWN</code> 状态.</p><h4 id=sdown---odwn>SDOWN -> ODWN?</h4><p><code>sentinel</code> 没有使用强一致性的算法来保证 <code>SDOWN</code> -> <code>ODOWN</code> 的转换,而是使用的<a href=https://zhuanlan.zhihu.com/p/41228196>Gossip协议</a>来保证最终一致性.在给定的时间范围内,给定的 <code>sentinel</code> 节点收到了足够多(<code>quorum</code>)的其它 <code>sentinel</code> 节点的 <code>SDOWN</code> 确认,就会从 <code>SDOWN</code> 切换到 <code>ODOWN</code> 了.</p><p>真正执行故障转移时会有比较严格的授权,但是前提也得是 <code>ODOWN</code> 状态才行.<code>ODOWN</code> 只针对 <code>master</code> 节点,<code>replicas</code> 和 <code>sentinels</code> 只会有 <code>SDOWN</code> 状态.如果 <code>replica</code> 变为 <code>SDOWN</code> 了,在故障转移的时候就不会被晋升.</p><h4 id=自动发现-auto-discovery>自动发现 auto discovery</h4><p><code>sentinel</code> 节点之间会保持连接来互相检查是否可用,交换信息,但是并不需要在启动的时候配置一长串其他 <code>sentinel</code> 节点的地址. <code>sentinel</code> 会利用 <code>redis</code> 的 <code>Pub/Sub</code> 能力来发现监控了相同 <code>master/replicas</code> 的 <code>sentinel</code> 节点.<code>replicas</code> 自动发现是一样的原理.</p><h5 id=如何实现的>如何实现的?</h5><p>向一个叫 <code>__sentinel__:hello</code> 的 <code>channel</code> 发送 hello 消息.</p><ul><li>每个 <code>sentinel</code> 节点都会向每一个它监控的 <code>master</code> 和 <code>replica</code> 的叫做 <code>__sentinel__:hello</code> 的Pub/Sub channel 广播自己的 <code>ip</code>,<code>port</code>,<code>runid</code>.2s 一次.</li><li>每个订阅了 <code>master</code> 和 <code>replica</code> 的 <code>sentinel</code> 都会收到消息,并会去判断有新的 <code>sentinel</code> 节点就会被添加进来.</li><li>这个 <code>hello</code> 消息同样包含着最新的 <code>master</code> 全量配置,每个收到消息的 <code>sentinel</code> 会进行比对更新.</li><li>添加新的 <code>sentinel</code> 节点时会提前判断该节点信息是否已经存在.</li></ul><h4 id=sentinel-强制更新配置>sentinel 强制更新配置</h4><p><strong>sentinel 是一个总会尝试将当前最新的配置强制更新到所有监控节点的系统</strong>.</p><blockquote><p>这可能也是一种 tradeoff 吧.比如 replica 如果连错 master 了,那 sentinel 就必须把它矫正过来,重连正确的master.</p></blockquote><h4 id=副本选举>副本选举</h4><p><code>sentinel</code> 可以执行故障转移时,需要选择一个合适的 <code>replica</code> 晋升.</p><h5 id=评估条件>评估条件</h5><ul><li>与 <code>master</code> 的断连时间</li><li><code>replica</code> 优先级->可以设置</li><li>复制进度 <code>offset</code></li><li>Run ID</li></ul><h5 id=判定需要跳过的节点>判定需要跳过的节点</h5><div class=highlight><pre style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#719e07>(</span>down-after-milliseconds * 10<span style=color:#719e07>)</span> + milliseconds_since_master_is_in_SDOWN_state
</code></pre></div><p>如果一个 <code>replica</code> 的断连时间超过上面这个表达式,那就认为该节点不可靠,不考虑. <code>down-after-milliseconds</code> 是通过设置的,<code>milliseconds_since_master_is_in_SDOWN_state</code> 指在执行故障转移时 <code>master</code> 仍不可用的时间.</p><h5 id=选举过程>选举过程</h5><p>符合上述条件后才会对其按照条件进行排序.顺序如下:</p><ul><li>首先根据 <code>replica-priority</code> 排序(<code>redis.conf</code> 进行设置),值越小越优先</li><li><code>priority</code> 相同时,比较 <code>offset</code>,值越大越优先(同步最完整)</li><li>如果 <code>priority</code>,<code>offset</code> 都相同,就会判断 <code>run ID</code> 的字典序.越小的 <code>run ID</code> 并不是说有什么优势,但是比起重排序随机选一个 <code>replica</code>,字典序选举方式更有确定性更有用(大白话).</li></ul><p>建议所有节点都设置 <code>replica-priority</code>.如果 <code>replica-priority</code> 设置为 0, 表示永远不会被选为 <code>master</code> .但是在故障转移后 <code>sentinel</code> 会重置通过这种方式设置的配置,以便可以与新的 <code>master</code> 连接,唯一的区别就是该节点不会是主节点.</p><h3 id=深入算法内部>深入算法内部</h3><h4 id=quorum>Quorum</h4><p><code>quorum</code> 参数会被 <code>sentinel</code> 集群用来判断是否有这个数量的 <code>sentinel</code> 节点认为 <code>master</code> 已经 <code>SDOWN</code> 了,需不需要转为 <code>ODOWN</code> 触发故障转移 <code>failover</code>.</p><p>但是,触发故障转移后,至少需要有<strong>半数</strong>的 <code>sentinel</code> 节点(如果 <code>quorum</code> 值比半数还多,那其实需要有<code>quorum</code>个节点)授权给一个 <code>sentinel</code> 节点才能真正执行.小于半数节点不允许执行.</p><blockquote><p>e.g. 5 instances <code>quorum</code> = 2</p><p>当有2个节点认为 <code>master</code> 不可达时,就会触发 failover.但是需要有至少3个节点授权给这2个节点之一才能真正执行failover.</p><p>如果 <code>quorum</code> = 5,那就需要所有节点都认为 <code>master</code> 不可达,才能触发failover,并且所有节点都要授权.</p></blockquote><h4 id=纪元-configuration-epochs>纪元 Configuration Epochs</h4><p>为什么需要获取半数以上的授权执行 <code>failover</code>?</p><p>当一个 <code>sentinel</code> 节点被授权后,会获得一个可以用于故障转移节点的唯一的纪元(<code>configuration epoch</code>)标志.这是一个在故障转移完成后针对新配置的版本号 number.因为是多数同意将指定的版本分配给指定授权的 <code>sentinel</code> ,所以不会有其他节点使用这个版本号.也就意味着每一次故障转移时生成的新配置都有唯一的版本号标识.</p><p><code>sentinel</code> 集群有一条规则: 如果 sentinel A 投票给 sentinel B 去执行故障转移,A 会等待一段时间后对同一个主节点再次进行故障转移.这个时间可以通过 <code>sentinel.conf</code> 的 <code>failover-timeout</code> 进行配置.这就意味着不会有节点在同一时间对同一个主节点进行故障转移,被授权的节点回先执行,失败了后面会有其他的节点进行重试.</p><p><code>sentinel</code> 保证 <a href=https://en.wikipedia.org/wiki/Liveness>liveness</a> 特性(我的理解就是不会宕机一直存活):如果有多个节点可用,只会选择一个节点去执行故障转移.</p><p><code>sentinel</code> 同样保证 <a href=https://en.wikipedia.org/wiki/Safety#System_safety_and_reliability_engineering>safety</a> 特性:每一个节点都会尝试使用不同的 <code>configuration epoch</code> 对相同的节点进行故障转移.</p><h4 id=配置传递--configuration-propagation>配置传递 Configuration propagation</h4><p>故障转移完成后,<code>sentinel</code> 会广播新的配置给其他 <code>sentinel</code> 节点更新这个新的主节点信息.执行故障转移的主节点还需要对新的主节点执行 <code>SLAVE NO ONE</code>,稍后在 <code>INFO</code> 命令中就可以看到这个主节点了.</p><p>所有 <code>sentinel</code> 节点都会广播配置信息,通过 <code>__sentinel__:hello</code> channel 广播出去.配置信息都带有 <code>epoch</code> ,值越大越会被当做最新的配置.</p><h4 id=网络分区后的一致性问题>网络分区后的一致性问题</h4><p>Redis + Sentinel 架构是<strong>保证最终一致性</strong>的系统,在发生网络分区恢复时,不可避免的会丢失数据.</p><p>如果把 redis 当做缓存来用,数据丢了也没事,可以再去库里查嘛.</p><p>如果把 redis 当做存储来用,那最好配上下面两个配置降低损失.</p><div class=highlight><pre style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>min-replicas-to-write <span style=color:#2aa198>1</span>
min-replicas-max-lag <span style=color:#2aa198>10</span>
</code></pre></div><h4 id=sentinel-状态持久化>Sentinel 状态持久化</h4><p>Sentinel 状态持久化在 <code>sentinel.conf</code> 中,每次手挡新配置,或者创建配置,都会带着<code>configuration epoch</code> 一起持久化到硬盘,重启时就没有问题了.</p></div><div class=post-tags><nav class="nav tags"><ul class=flat><li><a href=/tags/redis>redis</a></li></ul></nav></div><script src=https://utteranc.es/client.js repo=xiaoheiAh/xiaoheiAh.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><div class="footer wrapper"><nav class=nav><div class=badge><img src=https://img.shields.io/badge/PV-6188-green alt=pv>
<img src=https://img.shields.io/badge/UV-1747-green alt=uv>
<img src="https://img.shields.io/badge/License-CC%20BY%20NC%20ND%204.0-green?link=http://creativecommons.org/licenses/by-nc-nd/4.0/" alt="CC BY NC ND 4.0">
<span>| © 2019 | <a href=https://github.com/vividvilla/ezhil>Ezhil theme</a> | Built with <a href=https://gohugo.io>Hugo</a></span></div></nav></div><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-98254666-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script>feather.replace()</script></body></html>